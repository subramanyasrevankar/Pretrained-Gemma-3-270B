Gamma 270M & 270B â€“ Pretraining Pipeline

This project documents the Gamma 270M / 270B model pretraining process, inspired by Vizuraâ€™s tutorials and implemented step by step.

ğŸ“š Dataset

Tiny Stories dataset (~2M rows, 20K stories)

Tokenized and stored in train.bin and val.bin using np.memmap for fast loading.

ğŸ§  Tokenization

Character-level: simple but inefficient (ballooning effect, loses semantics).

Word-level: limited vocabulary, fails on typos.

Subword Tokenization (BPE) âœ…: merges frequent characters, efficient and robust.

ğŸ”„ Inputâ€“Output Pairs

Converts token sequences into next-token prediction targets.

Example: [1, 11, 15, 24] â†’ [11, 15, 24, 63]

Uses pinned memory + non-blocking transfer for training efficiency.

ğŸ— Model Architecture

Gamma uses 18 Transformer blocks (3 full attention + 15 sliding attention).

Key Components

Token Embeddings â†’ convert token IDs to trainable vectors

RMSNorm â†’ normalizes activations

Multi-Query & Sliding Attention â†’ efficient context handling

Rotary Positional Encoding (RoPE) â†’ adds position information

Feed Forward Network â†’ expands/activates dimensions (640 â†’ 2048 â†’ 640)

Output Projection â†’ maps back to vocabulary space

âš™ï¸ Training

Loss: Cross-entropy on next-token prediction

Optimizer: Adam + learning rate warmup/decay

Mixed precision training (FP16/FP32)

Gradient accumulation to simulate large batch sizes

ğŸ–¥ Inference

Autoregressive text generation: model predicts next token, appends it, and continues until completion.
